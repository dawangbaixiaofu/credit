{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "from sklearn import tree\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import scipy\n",
    "import json\n",
    "\n",
    "\n",
    "class Config:\n",
    "    engine = create_engine('mysql+pymysql://root:123@localhost:3307/give_me_some_credit')\n",
    "    train = 'cs_training'\n",
    "    label_field = 'SeriousDlqin2yrs'\n",
    "    encoded_train = 'cs_training_encoded'\n",
    "    train_dict_file = r\"..\\..\\encoders\\give_me_some_credit\\encoded_train_dict.json\"\n",
    "    # TODO: add log function\n",
    "    log_file = None\n",
    "    feature_importance_file = r\"\\feature_importance.json\"\n",
    "\n",
    "    test = 'cs_test'\n",
    "    encoded_test = 'cs_test_encoded'\n",
    "    test_dict_file = r\"..\\..\\encoders\\give_me_some_credit\\encoded_test_dict.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def get_train_sample():\n",
    "    label = pd.read_sql(sql=f\"select {Config.label_field} from {Config.train}\", con=Config.engine)\n",
    "    encoded_data = pd.read_sql(sql=f\"select * from {Config.encoded_train}\", con=Config.engine)\n",
    "    x_train, x_test, y_tran, y_test = train_test_split(encoded_data, label, train_size=0.8, random_state=10)\n",
    "    return x_train, x_test, y_tran, y_test\n",
    "\n",
    "\n",
    "def get_pred_sample():\n",
    "    id = pd.read_sql(sql=f\"select Id from {Config.test}\", con=Config.engine)\n",
    "    encoded_data = pd.read_sql(sql=f\"select * from {Config.encoded_test}\", con=Config.engine)\n",
    "    return id,encoded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tree model \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "import joblib\n",
    "import pickle\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'criterion':['gini', 'entropy', 'log_loss'],\n",
    "    'max_depth':[None, 8, 10, 12, 14],\n",
    "    'min_samples_split':[2, 8, 14],\n",
    "    'min_samples_leaf':[350, 400, 450],\n",
    "    'max_leaf_nodes':[None, 200, 400],\n",
    "    'random_state':[10,],\n",
    "    'class_weight':[None, 'balanced',],\n",
    "    # 'monotonic_cst':np.array([1, 0, -1,...)], \n",
    "}\n",
    "\n",
    "class ModelConfig:\n",
    "    cv_result = r\".\\cv_result.pkl\"\n",
    "    model_file = r\".\\clf.joblib\"\n",
    "    test_metrics = r\".\\test_metrics.pkl\"\n",
    "    pred_file = r\".\\sampleEntry.csv\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer():\n",
    "    x_train, x_test, y_train, y_test = get_train_sample()\n",
    "    estimator = DecisionTreeClassifier()\n",
    "    clf = GridSearchCV(estimator=estimator,\n",
    "                     param_grid=param_grid,\n",
    "                     scoring='roc_auc',\n",
    "                     n_jobs=-1,\n",
    "                     refit=True,\n",
    "                     cv=5,\n",
    "                     verbose=3,\n",
    "                     return_train_score=False\n",
    "                     )\n",
    "    clf.fit(x_train, y_train)\n",
    "\n",
    "    # save cross validate result using pickle file \n",
    "    with open(ModelConfig.cv_result, 'wb') as f:\n",
    "        pickle.dump(clf.cv_results_, f)\n",
    "    \n",
    "    # best params and socres on validations\n",
    "    print(f\"best params:\\n {clf.best_params_} \\n {clf.best_score_}\")\n",
    "    print(f\"best socres on validations:\\n {clf.best_score_}\")\n",
    "\n",
    "    # save best estimator by joblib format\n",
    "    joblib.dump(clf.best_estimator_, ModelConfig.model_file)\n",
    "\n",
    "    # test performance \n",
    "    test_metrics = {}\n",
    "    y_test_proba = clf.best_estimator_.predict_proba(x_test)\n",
    "    proba = list(map(lambda row: row[1], y_test_proba))\n",
    "    pred = clf.best_estimator_.predict(x_test)\n",
    "\n",
    "    test_metrics['auc'] = roc_auc_score(y_test, proba)\n",
    "    test_metrics['accuray'] = accuracy_score(y_test, pred)\n",
    "    test_metrics['precision'] = precision_score(y_test, pred)\n",
    "    test_metrics['recall'] = recall_score(y_test, pred)\n",
    "    test_metrics['f1'] = f1_score(y_test, pred)\n",
    "    \n",
    "    with open(ModelConfig.test_metrics, 'wb') as f:\n",
    "        pickle.dump(test_metrics, f)\n",
    "    print(f\"test metrics: \\n {test_metrics}\")\n",
    "\n",
    "\n",
    "def predict():\n",
    "    id, encoded_data = get_pred_sample()\n",
    "    m = joblib.load(ModelConfig.model_file)\n",
    "    proba = m.predict_proba(encoded_data)\n",
    "    proba = list(map(lambda row: row[1], proba))\n",
    "    id['Probability'] = proba\n",
    "    id.to_csv(ModelConfig.pred_file, index=False)\n",
    "    id.head(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 810 candidates, totalling 4050 fits\n",
      "best params:\n",
      " {'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_leaf_nodes': None, 'min_samples_leaf': 450, 'min_samples_split': 2, 'random_state': 10} \n",
      " 0.8537265605451033\n",
      "best socres on validations:\n",
      " 0.8537265605451033\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'x_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m      2\u001b[0m     clf \u001b[38;5;241m=\u001b[39m trainer()\n\u001b[1;32m----> 3\u001b[0m     \u001b[43msave_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     predict()\n",
      "Cell \u001b[1;32mIn[19], line 31\u001b[0m, in \u001b[0;36msave_result\u001b[1;34m(clf)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# test performance \u001b[39;00m\n\u001b[0;32m     30\u001b[0m test_metrics \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m---> 31\u001b[0m y_test_proba \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mbest_estimator_\u001b[38;5;241m.\u001b[39mpredict_proba(\u001b[43mx_test\u001b[49m)\n\u001b[0;32m     32\u001b[0m proba \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m row: row[\u001b[38;5;241m1\u001b[39m], y_test_proba))\n\u001b[0;32m     33\u001b[0m pred \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mbest_estimator_\u001b[38;5;241m.\u001b[39mpredict(x_test)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x_test' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    trainer()\n",
    "    predict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
