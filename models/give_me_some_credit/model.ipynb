{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "from sklearn import tree\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import scipy\n",
    "import json\n",
    "\n",
    "\n",
    "class Config:\n",
    "    engine = create_engine('mysql+pymysql://root:123@localhost:3307/give_me_some_credit')\n",
    "    train = 'cs_training'\n",
    "    label_field = 'SeriousDlqin2yrs'\n",
    "    encoded_train = 'cs_training_encoded'\n",
    "    train_dict_file = r\"..\\..\\encoders\\give_me_some_credit\\encoded_train_dict.json\"\n",
    "    # TODO: add log function\n",
    "    log_file = None\n",
    "    feature_importance_file = r\"\\feature_importance.json\"\n",
    "\n",
    "    test = 'cs_test'\n",
    "    encoded_test = 'cs_test_encoded'\n",
    "    test_dict_file = r\"..\\..\\encoders\\give_me_some_credit\\encoded_test_dict.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def get_train_sample():\n",
    "    label = pd.read_sql(sql=f\"select {Config.label_field} from {Config.train}\", con=Config.engine)\n",
    "    encoded_data = pd.read_sql(sql=f\"select * from {Config.encoded_train}\", con=Config.engine)\n",
    "    x_train, x_test, y_tran, y_test = train_test_split(encoded_data, label, train_size=0.8, random_state=10)\n",
    "    return x_train, x_test, y_tran, y_test\n",
    "\n",
    "\n",
    "def get_pred_sample():\n",
    "    id = pd.read_sql(sql=f\"select Id from {Config.test}\", con=Config.engine)\n",
    "    encoded_data = pd.read_sql(sql=f\"select * from {Config.encoded_test}\", con=Config.engine)\n",
    "    return id,encoded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tree model \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "import joblib\n",
    "import pickle\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'criterion':['gini', 'entropy', 'log_loss'],\n",
    "    'max_depth':[None, 8, 10, 12, 14],\n",
    "    'min_samples_split':[2, 8, 14],\n",
    "    'min_samples_leaf':[350, 400, 450],\n",
    "    'max_leaf_nodes':[None, 200, 400],\n",
    "    'random_state':[10,],\n",
    "    'class_weight':[None, 'balanced',],\n",
    "    # 'monotonic_cst':np.array([1, 0, -1,...)], \n",
    "}\n",
    "\n",
    "class ModelConfig:\n",
    "    cv_result = r\".\\cv_result.pkl\"\n",
    "    model_file = r\".\\clf.joblib\"\n",
    "    test_metrics = r\".\\test_metrics.pkl\"\n",
    "    pred_file = r\".\\sampleEntry.csv\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer():\n",
    "    x_train, x_test, y_train, y_test = get_train_sample()\n",
    "    estimator = DecisionTreeClassifier()\n",
    "    clf = GridSearchCV(estimator=estimator,\n",
    "                     param_grid=param_grid,\n",
    "                     scoring='roc_auc',\n",
    "                     n_jobs=-1,\n",
    "                     refit=True,\n",
    "                     cv=5,\n",
    "                     verbose=3,\n",
    "                     return_train_score=False\n",
    "                     )\n",
    "    clf.fit(x_train, y_train)\n",
    "\n",
    "    # save cross validate result using pickle file \n",
    "    with open(ModelConfig.cv_result, 'wb') as f:\n",
    "        pickle.dump(clf.cv_results_, f)\n",
    "    \n",
    "    # best params and socres on validations\n",
    "    print(f\"best params:\\n {clf.best_params_} \\n {clf.best_score_}\")\n",
    "    print(f\"best socres on validations:\\n {clf.best_score_}\")\n",
    "\n",
    "    # save best estimator by joblib format\n",
    "    joblib.dump(clf.best_estimator_, ModelConfig.model_file)\n",
    "\n",
    "    # test performance \n",
    "    test_metrics = {}\n",
    "    y_test_proba = clf.best_estimator_.predict_proba(x_test)\n",
    "    proba = list(map(lambda row: row[1], y_test_proba))\n",
    "    pred = clf.best_estimator_.predict(x_test)\n",
    "\n",
    "    test_metrics['auc'] = roc_auc_score(y_test, proba)\n",
    "    test_metrics['accuray'] = accuracy_score(y_test, pred)\n",
    "    test_metrics['precision'] = precision_score(y_test, pred)\n",
    "    test_metrics['recall'] = recall_score(y_test, pred)\n",
    "    test_metrics['f1'] = f1_score(y_test, pred)\n",
    "    \n",
    "    with open(ModelConfig.test_metrics, 'wb') as f:\n",
    "        pickle.dump(test_metrics, f)\n",
    "    print(f\"test metrics: \\n {test_metrics}\")\n",
    "\n",
    "\n",
    "def predict():\n",
    "    id, encoded_data = get_pred_sample()\n",
    "    m = joblib.load(ModelConfig.model_file)\n",
    "    proba = m.predict_proba(encoded_data)\n",
    "    proba = list(map(lambda row: row[1], proba))\n",
    "    id['Probability'] = proba\n",
    "    id.to_csv(ModelConfig.pred_file, index=False)\n",
    "    id.head(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 810 candidates, totalling 4050 fits\n",
      "best params:\n",
      " {'class_weight': None, 'criterion': 'entropy', 'max_depth': 10, 'max_leaf_nodes': None, 'min_samples_leaf': 450, 'min_samples_split': 2, 'random_state': 10} \n",
      " 0.8537265605451033\n",
      "best socres on validations:\n",
      " 0.8537265605451033\n",
      "test metrics: \n",
      " {'auc': 0.8598003876955467, 'accuray': 0.9372666666666667, 'precision': 0.6033690658499234, 'recall': 0.19533961328705998, 'f1': 0.2951310861423221}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    trainer()\n",
    "    predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
